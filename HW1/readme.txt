data_preprocess_5M.py, data_preprocess_38M.py:
These two python scripts are used to split the large data into small train, test set. We also use a hashcounter to determine the data type and the categorical frequency.

sparkLR_38M.ipynb:
This uses pyspark to train logistic regression on 5M data, then test on 38M data.( split to 5 part)

sparkLR.ipynb:
pyspark to train Logistic regression on 5M data and plot roc and auc.

sparkLR_test.ipynb:
train several models on each single dimension of data, pick ones with higher accuracy.

sparkRF.ipynb:
pyspark to train Random forest on 5M data and plot roc and auc. (roc reference: http://stackoverflow.com/questions/28818692/pyspark-mllib-class-probabilities-of-random-forest-predictions)

test.py:
use to build categorical dictionary and counter

cut5k.py:
use to cut small partition of data to do fast test